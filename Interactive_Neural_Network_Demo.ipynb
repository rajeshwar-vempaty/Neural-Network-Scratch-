{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Interactive Neural Network Visualization\n",
    "\n",
    "## Inspired by 3Blue1Brown's Neural Network Explanations\n",
    "\n",
    "This notebook provides an **interactive visualization** of how neural networks work from scratch. You'll be able to:\n",
    "\n",
    "1. **See the network architecture** - Watch neurons and connections in real-time\n",
    "2. **Understand forward propagation** - See how data flows through layers\n",
    "3. **Visualize weights** - Watch weights change as the network learns\n",
    "4. **Provide your own inputs** - Test the network with custom values\n",
    "5. **Train step-by-step** - Watch the network learn one step at a time\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install numpy matplotlib ipywidgets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle, Rectangle, FancyBboxPatch\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.colors as mcolors\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable interactive matplotlib in Jupyter\n",
    "%matplotlib widget\n",
    "\n",
    "print(\"‚úÖ All packages loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Neural Network from Scratch\n",
    "\n",
    "Let's build a flexible neural network class that stores all intermediate values for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractiveNeuralNetwork:\n",
    "    \"\"\"\n",
    "    A neural network implementation designed for interactive visualization.\n",
    "    Stores all intermediate activations, gradients, and weight updates.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, activation='sigmoid'):\n",
    "        \"\"\"\n",
    "        Initialize the neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        layer_sizes : list\n",
    "            Number of neurons in each layer. \n",
    "            Example: [4, 8, 4, 1] = 4 inputs, 2 hidden layers (8, 4), 1 output\n",
    "        activation : str\n",
    "            Activation function: 'sigmoid', 'relu', or 'tanh'\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        self.activation_name = activation\n",
    "        \n",
    "        # Initialize weights using Xavier initialization\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(self.num_layers - 1):\n",
    "            # Xavier initialization for better convergence\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2.0 / (layer_sizes[i] + layer_sizes[i+1]))\n",
    "            b = np.zeros((1, layer_sizes[i+1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "        \n",
    "        # Storage for visualization\n",
    "        self.activations = []      # Activation at each layer\n",
    "        self.z_values = []         # Pre-activation values\n",
    "        self.gradients = []        # Gradients for each layer\n",
    "        self.weight_updates = []   # Weight updates from last step\n",
    "        \n",
    "        # Training history\n",
    "        self.loss_history = []\n",
    "        self.accuracy_history = []\n",
    "        self.weight_history = []\n",
    "        self.epoch = 0\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(self, a):\n",
    "        \"\"\"Derivative of sigmoid.\"\"\"\n",
    "        return a * (1 - a)\n",
    "    \n",
    "    def relu(self, z):\n",
    "        \"\"\"ReLU activation function.\"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, a):\n",
    "        \"\"\"Derivative of ReLU.\"\"\"\n",
    "        return (a > 0).astype(float)\n",
    "    \n",
    "    def tanh_activation(self, z):\n",
    "        \"\"\"Tanh activation function.\"\"\"\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    def tanh_derivative(self, a):\n",
    "        \"\"\"Derivative of tanh.\"\"\"\n",
    "        return 1 - a**2\n",
    "    \n",
    "    def activate(self, z, output_layer=False):\n",
    "        \"\"\"Apply activation function.\"\"\"\n",
    "        if output_layer:\n",
    "            return self.sigmoid(z)  # Always sigmoid for output\n",
    "        \n",
    "        if self.activation_name == 'sigmoid':\n",
    "            return self.sigmoid(z)\n",
    "        elif self.activation_name == 'relu':\n",
    "            return self.relu(z)\n",
    "        elif self.activation_name == 'tanh':\n",
    "            return self.tanh_activation(z)\n",
    "        return z\n",
    "    \n",
    "    def activate_derivative(self, a):\n",
    "        \"\"\"Get derivative of activation.\"\"\"\n",
    "        if self.activation_name == 'sigmoid':\n",
    "            return self.sigmoid_derivative(a)\n",
    "        elif self.activation_name == 'relu':\n",
    "            return self.relu_derivative(a)\n",
    "        elif self.activation_name == 'tanh':\n",
    "            return self.tanh_derivative(a)\n",
    "        return np.ones_like(a)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation - compute activations for all layers.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray\n",
    "            Input data (n_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        ndarray : Output predictions\n",
    "        \"\"\"\n",
    "        self.activations = [X]\n",
    "        self.z_values = []\n",
    "        \n",
    "        current = X\n",
    "        for i in range(self.num_layers - 1):\n",
    "            z = np.dot(current, self.weights[i]) + self.biases[i]\n",
    "            self.z_values.append(z)\n",
    "            \n",
    "            is_output = (i == self.num_layers - 2)\n",
    "            current = self.activate(z, output_layer=is_output)\n",
    "            self.activations.append(current)\n",
    "        \n",
    "        return current\n",
    "    \n",
    "    def backward(self, X, y, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Backward propagation - compute gradients and update weights.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray\n",
    "            Input data\n",
    "        y : ndarray\n",
    "            True labels\n",
    "        learning_rate : float\n",
    "            Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        self.gradients = []\n",
    "        self.weight_updates = []\n",
    "        \n",
    "        # Output layer error\n",
    "        output = self.activations[-1]\n",
    "        delta = output - y\n",
    "        \n",
    "        # Backpropagate through layers\n",
    "        deltas = [delta]\n",
    "        for i in range(self.num_layers - 2, 0, -1):\n",
    "            delta = np.dot(delta, self.weights[i].T) * self.activate_derivative(self.activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        \n",
    "        self.gradients = deltas\n",
    "        \n",
    "        # Update weights and biases\n",
    "        for i in range(self.num_layers - 1):\n",
    "            weight_update = learning_rate * np.dot(self.activations[i].T, deltas[i]) / m\n",
    "            bias_update = learning_rate * np.sum(deltas[i], axis=0, keepdims=True) / m\n",
    "            \n",
    "            self.weight_updates.append(weight_update)\n",
    "            \n",
    "            self.weights[i] -= weight_update\n",
    "            self.biases[i] -= bias_update\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Binary cross-entropy loss.\"\"\"\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    def compute_accuracy(self, X, y):\n",
    "        \"\"\"Compute classification accuracy.\"\"\"\n",
    "        predictions = (self.forward(X) >= 0.5).astype(int)\n",
    "        return np.mean(predictions == y)\n",
    "    \n",
    "    def train_step(self, X, y, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Perform one training step.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple : (loss, accuracy)\n",
    "        \"\"\"\n",
    "        output = self.forward(X)\n",
    "        self.backward(X, y, learning_rate)\n",
    "        \n",
    "        loss = self.compute_loss(y, output)\n",
    "        accuracy = (np.mean((output >= 0.5).astype(int) == y))\n",
    "        \n",
    "        self.loss_history.append(loss)\n",
    "        self.accuracy_history.append(accuracy)\n",
    "        self.epoch += 1\n",
    "        \n",
    "        return loss, accuracy\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the network to initial random weights.\"\"\"\n",
    "        self.__init__(self.layer_sizes, self.activation_name)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def get_network_info(self):\n",
    "        \"\"\"Get information about the network.\"\"\"\n",
    "        total_params = sum(w.size + b.size for w, b in zip(self.weights, self.biases))\n",
    "        return {\n",
    "            'architecture': self.layer_sizes,\n",
    "            'activation': self.activation_name,\n",
    "            'total_parameters': total_params,\n",
    "            'epochs_trained': self.epoch\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ InteractiveNeuralNetwork class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Network Visualizer\n",
    "\n",
    "This creates beautiful 3Blue1Brown-style visualizations of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkVisualizer:\n",
    "    \"\"\"\n",
    "    3Blue1Brown-inspired neural network visualizer.\n",
    "    Creates beautiful, informative visualizations of neural networks.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Color scheme (3Blue1Brown inspired)\n",
    "    COLORS = {\n",
    "        'background': '#1a1a2e',\n",
    "        'positive': '#4ecdc4',      # Teal for positive weights\n",
    "        'negative': '#ff6b6b',      # Coral for negative weights\n",
    "        'neutral': '#95a5a6',       # Gray for neutral\n",
    "        'text': '#ecf0f1',          # Light text\n",
    "        'highlight': '#f39c12',     # Orange highlight\n",
    "        'activation_low': '#2c3e50',\n",
    "        'activation_high': '#3498db',\n",
    "        'input': '#9b59b6',         # Purple for input layer\n",
    "        'output': '#2ecc71',        # Green for output layer\n",
    "    }\n",
    "    \n",
    "    def __init__(self, network):\n",
    "        \"\"\"\n",
    "        Initialize the visualizer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        network : InteractiveNeuralNetwork\n",
    "            The neural network to visualize\n",
    "        \"\"\"\n",
    "        self.network = network\n",
    "        self.neuron_positions = self._compute_positions()\n",
    "    \n",
    "    def _compute_positions(self):\n",
    "        \"\"\"Compute x, y positions for each neuron.\"\"\"\n",
    "        positions = []\n",
    "        n_layers = len(self.network.layer_sizes)\n",
    "        \n",
    "        for layer_idx, n_neurons in enumerate(self.network.layer_sizes):\n",
    "            layer_pos = []\n",
    "            x = layer_idx / (n_layers - 1) if n_layers > 1 else 0.5\n",
    "            \n",
    "            for neuron_idx in range(n_neurons):\n",
    "                if n_neurons == 1:\n",
    "                    y = 0.5\n",
    "                else:\n",
    "                    y = 0.1 + 0.8 * (neuron_idx / (n_neurons - 1))\n",
    "                layer_pos.append((x, y))\n",
    "            positions.append(layer_pos)\n",
    "        \n",
    "        return positions\n",
    "    \n",
    "    def _weight_to_color(self, weight, max_weight):\n",
    "        \"\"\"Convert weight value to color.\"\"\"\n",
    "        if max_weight == 0:\n",
    "            max_weight = 1\n",
    "        \n",
    "        normalized = np.clip(weight / max_weight, -1, 1)\n",
    "        \n",
    "        if normalized >= 0:\n",
    "            alpha = 0.2 + 0.8 * normalized\n",
    "            return (*mcolors.to_rgb(self.COLORS['positive']), alpha)\n",
    "        else:\n",
    "            alpha = 0.2 + 0.8 * abs(normalized)\n",
    "            return (*mcolors.to_rgb(self.COLORS['negative']), alpha)\n",
    "    \n",
    "    def _activation_to_color(self, activation):\n",
    "        \"\"\"Convert activation value to color.\"\"\"\n",
    "        activation = np.clip(activation, 0, 1)\n",
    "        \n",
    "        # Interpolate between dark and bright blue\n",
    "        low = np.array(mcolors.to_rgb(self.COLORS['activation_low']))\n",
    "        high = np.array(mcolors.to_rgb(self.COLORS['activation_high']))\n",
    "        \n",
    "        color = low + activation * (high - low)\n",
    "        return (*color, 0.9)\n",
    "    \n",
    "    def draw_network(self, ax, input_values=None, show_weights=True, \n",
    "                     show_values=True, show_gradients=False, title=None):\n",
    "        \"\"\"\n",
    "        Draw the neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        ax : matplotlib axes\n",
    "            Axes to draw on\n",
    "        input_values : ndarray, optional\n",
    "            Input to propagate through network\n",
    "        show_weights : bool\n",
    "            Show weight connections\n",
    "        show_values : bool\n",
    "            Show activation values in neurons\n",
    "        show_gradients : bool\n",
    "            Show gradient values (if available)\n",
    "        title : str, optional\n",
    "            Title for the plot\n",
    "        \"\"\"\n",
    "        ax.clear()\n",
    "        ax.set_facecolor(self.COLORS['background'])\n",
    "        ax.set_xlim(-0.1, 1.1)\n",
    "        ax.set_ylim(-0.05, 1.05)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Forward pass if input provided\n",
    "        if input_values is not None:\n",
    "            if input_values.ndim == 1:\n",
    "                input_values = input_values.reshape(1, -1)\n",
    "            self.network.forward(input_values)\n",
    "        \n",
    "        # Get max weight for normalization\n",
    "        max_weight = max(np.abs(w).max() for w in self.network.weights) if self.network.weights else 1\n",
    "        \n",
    "        # Draw connections (weights)\n",
    "        if show_weights:\n",
    "            for layer_idx in range(len(self.network.weights)):\n",
    "                weights = self.network.weights[layer_idx]\n",
    "                \n",
    "                for i, pos1 in enumerate(self.neuron_positions[layer_idx]):\n",
    "                    for j, pos2 in enumerate(self.neuron_positions[layer_idx + 1]):\n",
    "                        weight = weights[i, j]\n",
    "                        color = self._weight_to_color(weight, max_weight)\n",
    "                        linewidth = 0.5 + 2.5 * abs(weight) / max_weight\n",
    "                        \n",
    "                        ax.plot([pos1[0], pos2[0]], [pos1[1], pos2[1]],\n",
    "                               color=color, linewidth=linewidth, zorder=1)\n",
    "        \n",
    "        # Draw neurons\n",
    "        neuron_radius = 0.03\n",
    "        \n",
    "        for layer_idx, layer_pos in enumerate(self.neuron_positions):\n",
    "            is_input = (layer_idx == 0)\n",
    "            is_output = (layer_idx == len(self.network.layer_sizes) - 1)\n",
    "            \n",
    "            for neuron_idx, (x, y) in enumerate(layer_pos):\n",
    "                # Get activation value\n",
    "                if self.network.activations and layer_idx < len(self.network.activations):\n",
    "                    act = self.network.activations[layer_idx]\n",
    "                    value = float(act[0, neuron_idx]) if act.ndim > 1 else float(act[neuron_idx])\n",
    "                    color = self._activation_to_color(value)\n",
    "                else:\n",
    "                    value = 0\n",
    "                    color = self._activation_to_color(0)\n",
    "                \n",
    "                # Special colors for input/output layers\n",
    "                if is_input:\n",
    "                    edge_color = self.COLORS['input']\n",
    "                elif is_output:\n",
    "                    edge_color = self.COLORS['output']\n",
    "                else:\n",
    "                    edge_color = 'white'\n",
    "                \n",
    "                # Draw neuron\n",
    "                circle = Circle((x, y), neuron_radius, facecolor=color,\n",
    "                              edgecolor=edge_color, linewidth=2, zorder=5)\n",
    "                ax.add_patch(circle)\n",
    "                \n",
    "                # Show value\n",
    "                if show_values and self.network.activations:\n",
    "                    ax.text(x, y, f'{value:.2f}', ha='center', va='center',\n",
    "                           fontsize=7, color='white', fontweight='bold', zorder=6)\n",
    "        \n",
    "        # Layer labels\n",
    "        layer_names = ['Input'] + [f'Hidden {i+1}' for i in range(len(self.network.layer_sizes) - 2)] + ['Output']\n",
    "        for idx, name in enumerate(layer_names):\n",
    "            x = idx / (len(self.network.layer_sizes) - 1) if len(self.network.layer_sizes) > 1 else 0.5\n",
    "            ax.text(x, -0.02, name, ha='center', va='top', fontsize=10,\n",
    "                   color=self.COLORS['text'], fontweight='bold')\n",
    "            ax.text(x, 1.02, f'{self.network.layer_sizes[idx]} neurons',\n",
    "                   ha='center', va='bottom', fontsize=8, color=self.COLORS['text'], alpha=0.7)\n",
    "        \n",
    "        # Title\n",
    "        if title:\n",
    "            ax.set_title(title, color=self.COLORS['text'], fontsize=14, fontweight='bold', pad=15)\n",
    "    \n",
    "    def draw_weights_heatmap(self, ax, layer_idx=0):\n",
    "        \"\"\"Draw weight matrix as heatmap.\"\"\"\n",
    "        ax.clear()\n",
    "        ax.set_facecolor(self.COLORS['background'])\n",
    "        \n",
    "        if layer_idx >= len(self.network.weights):\n",
    "            return\n",
    "        \n",
    "        weights = self.network.weights[layer_idx]\n",
    "        max_val = np.abs(weights).max()\n",
    "        \n",
    "        im = ax.imshow(weights.T, cmap='RdBu_r', aspect='auto',\n",
    "                      vmin=-max_val, vmax=max_val)\n",
    "        \n",
    "        ax.set_title(f'Weights: Layer {layer_idx} ‚Üí {layer_idx + 1}',\n",
    "                    color=self.COLORS['text'], fontsize=11)\n",
    "        ax.set_xlabel(f'From Layer {layer_idx}', color=self.COLORS['text'])\n",
    "        ax.set_ylabel(f'To Layer {layer_idx + 1}', color=self.COLORS['text'])\n",
    "        ax.tick_params(colors=self.COLORS['text'])\n",
    "        \n",
    "        return im\n",
    "    \n",
    "    def draw_training_progress(self, ax_loss, ax_acc):\n",
    "        \"\"\"Draw training curves.\"\"\"\n",
    "        # Loss curve\n",
    "        ax_loss.clear()\n",
    "        ax_loss.set_facecolor(self.COLORS['background'])\n",
    "        if self.network.loss_history:\n",
    "            ax_loss.plot(self.network.loss_history, color=self.COLORS['negative'], linewidth=2)\n",
    "        ax_loss.set_title('Loss', color=self.COLORS['text'], fontsize=10)\n",
    "        ax_loss.set_xlabel('Epoch', color=self.COLORS['text'], fontsize=8)\n",
    "        ax_loss.tick_params(colors=self.COLORS['text'])\n",
    "        ax_loss.grid(True, alpha=0.2)\n",
    "        \n",
    "        # Accuracy curve\n",
    "        ax_acc.clear()\n",
    "        ax_acc.set_facecolor(self.COLORS['background'])\n",
    "        if self.network.accuracy_history:\n",
    "            ax_acc.plot(self.network.accuracy_history, color=self.COLORS['output'], linewidth=2)\n",
    "        ax_acc.set_title('Accuracy', color=self.COLORS['text'], fontsize=10)\n",
    "        ax_acc.set_xlabel('Epoch', color=self.COLORS['text'], fontsize=8)\n",
    "        ax_acc.tick_params(colors=self.COLORS['text'])\n",
    "        ax_acc.grid(True, alpha=0.2)\n",
    "        ax_acc.set_ylim(0, 1)\n",
    "\n",
    "print(\"‚úÖ NetworkVisualizer class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Create Sample Data\n",
    "\n",
    "Let's create some data for our neural network to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_data(n_samples=200, n_features=4, random_state=42):\n",
    "    \"\"\"\n",
    "    Create sample binary classification data.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    n_per_class = n_samples // 2\n",
    "    \n",
    "    # Class 0: lower values\n",
    "    X0 = np.random.randn(n_per_class, n_features) * 0.5 - 0.5\n",
    "    \n",
    "    # Class 1: higher values\n",
    "    X1 = np.random.randn(n_per_class, n_features) * 0.5 + 0.5\n",
    "    \n",
    "    X = np.vstack([X0, X1])\n",
    "    y = np.array([0] * n_per_class + [1] * n_per_class).reshape(-1, 1)\n",
    "    \n",
    "    # Shuffle\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    \n",
    "    # Normalize to 0-1\n",
    "    X = (X - X.min()) / (X.max() - X.min())\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Create data\n",
    "X_train, y_train = create_sample_data(n_samples=200, n_features=4)\n",
    "\n",
    "print(f\"‚úÖ Created training data:\")\n",
    "print(f\"   - Samples: {X_train.shape[0]}\")\n",
    "print(f\"   - Features: {X_train.shape[1]}\")\n",
    "print(f\"   - Classes: {len(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Interactive Visualization Dashboard\n",
    "\n",
    "This is the main interactive interface! You can:\n",
    "- **Train** the network step-by-step or in batches\n",
    "- **Reset** to start fresh\n",
    "- **Input custom values** and see the prediction\n",
    "- **Watch weights change** as training progresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network\n",
    "nn = InteractiveNeuralNetwork(layer_sizes=[4, 6, 4, 1], activation='sigmoid')\n",
    "viz = NetworkVisualizer(nn)\n",
    "\n",
    "print(\"‚úÖ Neural Network created!\")\n",
    "print(f\"   Architecture: {nn.layer_sizes}\")\n",
    "print(f\"   Total parameters: {nn.get_network_info()['total_parameters']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive dashboard using ipywidgets\n",
    "\n",
    "# Output areas\n",
    "output_network = widgets.Output()\n",
    "output_info = widgets.Output()\n",
    "\n",
    "# Create the figure\n",
    "with output_network:\n",
    "    fig = plt.figure(figsize=(14, 10), facecolor='#1a1a2e')\n",
    "    \n",
    "    # Main network visualization\n",
    "    ax_network = fig.add_axes([0.05, 0.35, 0.55, 0.58])\n",
    "    \n",
    "    # Training curves\n",
    "    ax_loss = fig.add_axes([0.65, 0.55, 0.3, 0.18])\n",
    "    ax_acc = fig.add_axes([0.65, 0.78, 0.3, 0.15])\n",
    "    \n",
    "    # Weight matrix\n",
    "    ax_weights = fig.add_axes([0.65, 0.35, 0.3, 0.15])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def update_visualization(sample_idx=0, layer_idx=0):\n",
    "    \"\"\"Update the visualization.\"\"\"\n",
    "    with output_network:\n",
    "        # Get sample\n",
    "        input_val = X_train[sample_idx:sample_idx+1]\n",
    "        \n",
    "        # Draw network\n",
    "        viz.draw_network(ax_network, input_values=input_val, show_weights=True, show_values=True,\n",
    "                        title=f'Neural Network | Epoch: {nn.epoch} | Sample: {sample_idx}')\n",
    "        \n",
    "        # Draw training progress\n",
    "        viz.draw_training_progress(ax_loss, ax_acc)\n",
    "        \n",
    "        # Draw weight matrix\n",
    "        viz.draw_weights_heatmap(ax_weights, layer_idx)\n",
    "        \n",
    "        fig.canvas.draw_idle()\n",
    "\n",
    "# Create widgets\n",
    "learning_rate_slider = widgets.FloatSlider(\n",
    "    value=0.1, min=0.001, max=1.0, step=0.01,\n",
    "    description='Learning Rate:', style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "sample_slider = widgets.IntSlider(\n",
    "    value=0, min=0, max=len(X_train)-1, step=1,\n",
    "    description='Sample Index:', style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "layer_slider = widgets.IntSlider(\n",
    "    value=0, min=0, max=len(nn.weights)-1, step=1,\n",
    "    description='Weight Layer:', style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "epochs_input = widgets.IntText(\n",
    "    value=100, description='Epochs:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "custom_input = widgets.Text(\n",
    "    value='0.5, 0.5, 0.5, 0.5',\n",
    "    description='Custom Input:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "# Buttons\n",
    "train_button = widgets.Button(description='Train', button_style='success', layout=widgets.Layout(width='100px'))\n",
    "step_button = widgets.Button(description='Step', button_style='info', layout=widgets.Layout(width='100px'))\n",
    "reset_button = widgets.Button(description='Reset', button_style='danger', layout=widgets.Layout(width='100px'))\n",
    "forward_button = widgets.Button(description='Forward Pass', button_style='primary', layout=widgets.Layout(width='120px'))\n",
    "\n",
    "# Status label\n",
    "status_label = widgets.HTML(value='<b>Ready to train!</b>')\n",
    "\n",
    "def on_train_click(b):\n",
    "    \"\"\"Train for multiple epochs.\"\"\"\n",
    "    lr = learning_rate_slider.value\n",
    "    epochs = epochs_input.value\n",
    "    \n",
    "    status_label.value = f'<b>Training for {epochs} epochs...</b>'\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        loss, acc = nn.train_step(X_train, y_train, lr)\n",
    "    \n",
    "    status_label.value = f'<b>Epoch {nn.epoch} | Loss: {loss:.4f} | Accuracy: {acc:.4f}</b>'\n",
    "    update_visualization(sample_slider.value, layer_slider.value)\n",
    "\n",
    "def on_step_click(b):\n",
    "    \"\"\"Train for one step.\"\"\"\n",
    "    lr = learning_rate_slider.value\n",
    "    loss, acc = nn.train_step(X_train, y_train, lr)\n",
    "    \n",
    "    status_label.value = f'<b>Epoch {nn.epoch} | Loss: {loss:.4f} | Accuracy: {acc:.4f}</b>'\n",
    "    update_visualization(sample_slider.value, layer_slider.value)\n",
    "\n",
    "def on_reset_click(b):\n",
    "    \"\"\"Reset the network.\"\"\"\n",
    "    nn.reset()\n",
    "    status_label.value = '<b>Network reset! Ready to train.</b>'\n",
    "    update_visualization(sample_slider.value, layer_slider.value)\n",
    "\n",
    "def on_forward_click(b):\n",
    "    \"\"\"Forward pass with custom input.\"\"\"\n",
    "    try:\n",
    "        values = [float(x.strip()) for x in custom_input.value.split(',')]\n",
    "        if len(values) == nn.layer_sizes[0]:\n",
    "            input_arr = np.array([values])\n",
    "            output = nn.forward(input_arr)\n",
    "            \n",
    "            with output_network:\n",
    "                viz.draw_network(ax_network, input_values=input_arr, show_weights=True, show_values=True,\n",
    "                               title=f'Custom Input | Output: {output[0,0]:.4f}')\n",
    "                fig.canvas.draw_idle()\n",
    "            \n",
    "            prediction = 'Class 1' if output[0,0] >= 0.5 else 'Class 0'\n",
    "            status_label.value = f'<b>Output: {output[0,0]:.4f} ‚Üí {prediction}</b>'\n",
    "        else:\n",
    "            status_label.value = f'<b style=\"color:red\">Error: Need {nn.layer_sizes[0]} input values</b>'\n",
    "    except Exception as e:\n",
    "        status_label.value = f'<b style=\"color:red\">Error: {str(e)}</b>'\n",
    "\n",
    "def on_slider_change(change):\n",
    "    \"\"\"Handle slider changes.\"\"\"\n",
    "    update_visualization(sample_slider.value, layer_slider.value)\n",
    "\n",
    "# Connect callbacks\n",
    "train_button.on_click(on_train_click)\n",
    "step_button.on_click(on_step_click)\n",
    "reset_button.on_click(on_reset_click)\n",
    "forward_button.on_click(on_forward_click)\n",
    "sample_slider.observe(on_slider_change, names='value')\n",
    "layer_slider.observe(on_slider_change, names='value')\n",
    "\n",
    "# Layout\n",
    "controls_row1 = widgets.HBox([learning_rate_slider, epochs_input])\n",
    "controls_row2 = widgets.HBox([sample_slider, layer_slider])\n",
    "buttons_row = widgets.HBox([train_button, step_button, reset_button])\n",
    "custom_row = widgets.HBox([custom_input, forward_button])\n",
    "\n",
    "controls = widgets.VBox([\n",
    "    widgets.HTML('<h3>üéõÔ∏è Controls</h3>'),\n",
    "    controls_row1,\n",
    "    controls_row2,\n",
    "    buttons_row,\n",
    "    widgets.HTML('<h4>üî¨ Test Custom Input</h4>'),\n",
    "    custom_row,\n",
    "    widgets.HTML('<hr>'),\n",
    "    status_label\n",
    "])\n",
    "\n",
    "# Initial visualization\n",
    "update_visualization(0, 0)\n",
    "\n",
    "# Display\n",
    "display(controls)\n",
    "display(output_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé¨ Step-by-Step Forward Propagation Visualization\n",
    "\n",
    "Watch how data flows through each layer of the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_forward_step_by_step(network, input_values, delay=1.0):\n",
    "    \"\"\"\n",
    "    Visualize forward propagation step by step.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    network : InteractiveNeuralNetwork\n",
    "        The neural network\n",
    "    input_values : ndarray\n",
    "        Input values (should be 1D or 2D with shape (1, n_features))\n",
    "    delay : float\n",
    "        Delay between steps in seconds\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from IPython.display import clear_output\n",
    "    \n",
    "    if input_values.ndim == 1:\n",
    "        input_values = input_values.reshape(1, -1)\n",
    "    \n",
    "    viz_local = NetworkVisualizer(network)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6), facecolor='#1a1a2e')\n",
    "    ax_net = axes[0]\n",
    "    ax_info = axes[1]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  FORWARD PROPAGATION VISUALIZATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nInput: {input_values[0]}\\n\")\n",
    "    \n",
    "    # Step through each layer\n",
    "    current = input_values\n",
    "    network.activations = [current]\n",
    "    \n",
    "    for layer_idx in range(network.num_layers - 1):\n",
    "        # Compute this layer\n",
    "        z = np.dot(current, network.weights[layer_idx]) + network.biases[layer_idx]\n",
    "        \n",
    "        is_output = (layer_idx == network.num_layers - 2)\n",
    "        if is_output:\n",
    "            current = network.sigmoid(z)\n",
    "        else:\n",
    "            current = network.activate(z)\n",
    "        \n",
    "        network.activations.append(current)\n",
    "        \n",
    "        # Clear and redraw\n",
    "        ax_net.clear()\n",
    "        ax_info.clear()\n",
    "        \n",
    "        # Draw network\n",
    "        viz_local.draw_network(ax_net, show_weights=True, show_values=True,\n",
    "                              title=f'Layer {layer_idx} ‚Üí Layer {layer_idx + 1}')\n",
    "        \n",
    "        # Info panel\n",
    "        ax_info.set_facecolor('#1a1a2e')\n",
    "        ax_info.axis('off')\n",
    "        \n",
    "        info_text = f\"\"\"\n",
    "        LAYER {layer_idx + 1} COMPUTATION\n",
    "        {'='*30}\n",
    "        \n",
    "        Input shape: {network.activations[layer_idx].shape}\n",
    "        Weight shape: {network.weights[layer_idx].shape}\n",
    "        \n",
    "        z = input √ó weights + bias\n",
    "        a = activation(z)\n",
    "        \n",
    "        Output: {current[0]}\n",
    "        \"\"\"\n",
    "        \n",
    "        ax_info.text(0.1, 0.5, info_text, transform=ax_info.transAxes,\n",
    "                    fontsize=11, color='white', family='monospace',\n",
    "                    verticalalignment='center')\n",
    "        \n",
    "        fig.canvas.draw()\n",
    "        plt.pause(delay)\n",
    "        \n",
    "        print(f\"Layer {layer_idx + 1}: {current[0]}\")\n",
    "    \n",
    "    # Final result\n",
    "    output = network.activations[-1][0, 0]\n",
    "    prediction = \"Class 1\" if output >= 0.5 else \"Class 0\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FINAL OUTPUT: {output:.4f}\")\n",
    "    print(f\"PREDICTION: {prediction}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    plt.show()\n",
    "    return output\n",
    "\n",
    "print(\"‚úÖ Forward propagation visualizer ready!\")\n",
    "print(\"Run the next cell to see step-by-step propagation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Visualize forward propagation step by step\n",
    "test_input = np.array([0.2, 0.8, 0.5, 0.3])\n",
    "print(f\"Testing with input: {test_input}\")\n",
    "\n",
    "output = visualize_forward_step_by_step(nn, test_input, delay=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Weight Evolution Visualization\n",
    "\n",
    "Watch how weights evolve during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_evolution(layer_sizes=[4, 6, 4, 1], n_epochs=500, lr=0.1, \n",
    "                                  snapshot_every=50):\n",
    "    \"\"\"\n",
    "    Visualize how the network evolves during training.\n",
    "    \"\"\"\n",
    "    # Create fresh network\n",
    "    net = InteractiveNeuralNetwork(layer_sizes, activation='sigmoid')\n",
    "    \n",
    "    # Store weight snapshots\n",
    "    weight_snapshots = []\n",
    "    epochs_recorded = []\n",
    "    \n",
    "    print(f\"Training for {n_epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        loss, acc = net.train_step(X_train, y_train, lr)\n",
    "        \n",
    "        if epoch % snapshot_every == 0:\n",
    "            weight_snapshots.append([w.copy() for w in net.weights])\n",
    "            epochs_recorded.append(epoch)\n",
    "            print(f\"  Epoch {epoch}: Loss={loss:.4f}, Accuracy={acc:.4f}\")\n",
    "    \n",
    "    # Add final snapshot\n",
    "    weight_snapshots.append([w.copy() for w in net.weights])\n",
    "    epochs_recorded.append(n_epochs)\n",
    "    \n",
    "    # Visualize snapshots\n",
    "    n_snapshots = len(weight_snapshots)\n",
    "    n_layers = len(layer_sizes) - 1\n",
    "    \n",
    "    fig, axes = plt.subplots(n_layers, min(n_snapshots, 6), figsize=(16, 4*n_layers),\n",
    "                            facecolor='#1a1a2e')\n",
    "    \n",
    "    if n_layers == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    # Select subset of snapshots to show\n",
    "    indices = np.linspace(0, n_snapshots-1, min(n_snapshots, 6), dtype=int)\n",
    "    \n",
    "    for row, layer_idx in enumerate(range(n_layers)):\n",
    "        for col, snap_idx in enumerate(indices):\n",
    "            ax = axes[row, col] if n_layers > 1 else axes[col]\n",
    "            ax.set_facecolor('#1a1a2e')\n",
    "            \n",
    "            weights = weight_snapshots[snap_idx][layer_idx]\n",
    "            max_val = max(np.abs(w).max() for snapshot in weight_snapshots for w in snapshot)\n",
    "            \n",
    "            im = ax.imshow(weights.T, cmap='RdBu_r', aspect='auto',\n",
    "                          vmin=-max_val, vmax=max_val)\n",
    "            \n",
    "            ax.set_title(f'Epoch {epochs_recorded[snap_idx]}', color='white', fontsize=10)\n",
    "            \n",
    "            if col == 0:\n",
    "                ax.set_ylabel(f'Layer {layer_idx}‚Üí{layer_idx+1}', color='white')\n",
    "            \n",
    "            ax.tick_params(colors='white')\n",
    "    \n",
    "    plt.suptitle('Weight Evolution During Training', color='white', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Also show training curves\n",
    "    fig2, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4), facecolor='#1a1a2e')\n",
    "    \n",
    "    ax1.set_facecolor('#1a1a2e')\n",
    "    ax1.plot(net.loss_history, color='#ff6b6b', linewidth=2)\n",
    "    ax1.set_title('Loss During Training', color='white')\n",
    "    ax1.set_xlabel('Epoch', color='white')\n",
    "    ax1.set_ylabel('Loss', color='white')\n",
    "    ax1.tick_params(colors='white')\n",
    "    ax1.grid(True, alpha=0.2)\n",
    "    \n",
    "    ax2.set_facecolor('#1a1a2e')\n",
    "    ax2.plot(net.accuracy_history, color='#2ecc71', linewidth=2)\n",
    "    ax2.set_title('Accuracy During Training', color='white')\n",
    "    ax2.set_xlabel('Epoch', color='white')\n",
    "    ax2.set_ylabel('Accuracy', color='white')\n",
    "    ax2.tick_params(colors='white')\n",
    "    ax2.grid(True, alpha=0.2)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return net\n",
    "\n",
    "print(\"‚úÖ Weight evolution visualizer ready!\")\n",
    "print(\"Run the next cell to train and visualize weight evolution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and visualize weight evolution\n",
    "trained_net = visualize_training_evolution(\n",
    "    layer_sizes=[4, 6, 4, 1],\n",
    "    n_epochs=500,\n",
    "    lr=0.2,\n",
    "    snapshot_every=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Interactive Custom Network Builder\n",
    "\n",
    "Build your own network architecture and watch it learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive network builder\n",
    "@interact(\n",
    "    hidden1=widgets.IntSlider(min=2, max=10, value=6, description='Hidden 1:'),\n",
    "    hidden2=widgets.IntSlider(min=2, max=10, value=4, description='Hidden 2:'),\n",
    "    activation=widgets.Dropdown(options=['sigmoid', 'relu', 'tanh'], value='sigmoid', description='Activation:'),\n",
    "    learning_rate=widgets.FloatSlider(min=0.01, max=1.0, value=0.1, step=0.01, description='LR:'),\n",
    "    epochs=widgets.IntSlider(min=100, max=2000, value=500, step=100, description='Epochs:')\n",
    ")\n",
    "def build_and_train(hidden1, hidden2, activation, learning_rate, epochs):\n",
    "    \"\"\"Build and train a custom network.\"\"\"\n",
    "    \n",
    "    # Create network with custom architecture\n",
    "    layer_sizes = [4, hidden1, hidden2, 1]\n",
    "    net = InteractiveNeuralNetwork(layer_sizes, activation=activation)\n",
    "    \n",
    "    print(f\"\\nNetwork Architecture: {layer_sizes}\")\n",
    "    print(f\"Activation: {activation}\")\n",
    "    print(f\"Training for {epochs} epochs with LR={learning_rate}...\\n\")\n",
    "    \n",
    "    # Train\n",
    "    for _ in range(epochs):\n",
    "        loss, acc = net.train_step(X_train, y_train, learning_rate)\n",
    "    \n",
    "    print(f\"Final Loss: {loss:.4f}\")\n",
    "    print(f\"Final Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Visualize\n",
    "    viz = NetworkVisualizer(net)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5), facecolor='#1a1a2e')\n",
    "    \n",
    "    # Network structure\n",
    "    viz.draw_network(axes[0], input_values=X_train[0:1], show_weights=True, show_values=True,\n",
    "                    title='Network Architecture')\n",
    "    \n",
    "    # Loss curve\n",
    "    axes[1].set_facecolor('#1a1a2e')\n",
    "    axes[1].plot(net.loss_history, color='#ff6b6b', linewidth=2)\n",
    "    axes[1].set_title('Loss', color='white')\n",
    "    axes[1].set_xlabel('Epoch', color='white')\n",
    "    axes[1].tick_params(colors='white')\n",
    "    axes[1].grid(True, alpha=0.2)\n",
    "    \n",
    "    # Accuracy curve\n",
    "    axes[2].set_facecolor('#1a1a2e')\n",
    "    axes[2].plot(net.accuracy_history, color='#2ecc71', linewidth=2)\n",
    "    axes[2].set_title('Accuracy', color='white')\n",
    "    axes[2].set_xlabel('Epoch', color='white')\n",
    "    axes[2].tick_params(colors='white')\n",
    "    axes[2].grid(True, alpha=0.2)\n",
    "    axes[2].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Understanding Neurons: Activation Exploration\n",
    "\n",
    "See how different neurons respond to different inputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_neuron_activations(network, n_samples=100):\n",
    "    \"\"\"\n",
    "    Explore how neurons activate across different inputs.\n",
    "    \"\"\"\n",
    "    # Generate random inputs\n",
    "    np.random.seed(42)\n",
    "    inputs = np.random.rand(n_samples, network.layer_sizes[0])\n",
    "    \n",
    "    # Collect activations\n",
    "    all_activations = []\n",
    "    for i in range(n_samples):\n",
    "        network.forward(inputs[i:i+1])\n",
    "        all_activations.append([a.copy() for a in network.activations])\n",
    "    \n",
    "    # Visualize activation distributions for each layer\n",
    "    n_layers = len(network.layer_sizes)\n",
    "    fig, axes = plt.subplots(1, n_layers, figsize=(4*n_layers, 4), facecolor='#1a1a2e')\n",
    "    \n",
    "    for layer_idx in range(n_layers):\n",
    "        ax = axes[layer_idx]\n",
    "        ax.set_facecolor('#1a1a2e')\n",
    "        \n",
    "        # Get activations for this layer across all samples\n",
    "        layer_acts = np.array([act[layer_idx][0] for act in all_activations])\n",
    "        \n",
    "        # Box plot for each neuron\n",
    "        bp = ax.boxplot(layer_acts, patch_artist=True)\n",
    "        \n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, network.layer_sizes[layer_idx]))\n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "        \n",
    "        ax.set_title(f'Layer {layer_idx}', color='white')\n",
    "        ax.set_xlabel('Neuron', color='white')\n",
    "        ax.set_ylabel('Activation', color='white')\n",
    "        ax.tick_params(colors='white')\n",
    "        ax.grid(True, alpha=0.2)\n",
    "    \n",
    "    plt.suptitle('Neuron Activation Distributions Across Inputs', color='white', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# First train the network if needed\n",
    "if nn.epoch < 100:\n",
    "    print(\"Training network first...\")\n",
    "    for _ in range(200):\n",
    "        nn.train_step(X_train, y_train, 0.1)\n",
    "\n",
    "print(f\"\\nExploring activations after {nn.epoch} training epochs:\")\n",
    "explore_neuron_activations(nn, n_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Educational Summary\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **Network Architecture**: How neurons are organized in layers\n",
    "2. **Forward Propagation**: How data flows through the network\n",
    "3. **Weights & Biases**: How they determine the network's behavior\n",
    "4. **Activation Functions**: How neurons \"fire\" based on their inputs\n",
    "5. **Training**: How the network learns by adjusting weights\n",
    "6. **Loss & Accuracy**: How we measure the network's performance\n",
    "\n",
    "### Key Concepts Visualized:\n",
    "\n",
    "- **Blue connections** = Positive weights (enhance the signal)\n",
    "- **Red connections** = Negative weights (inhibit the signal)\n",
    "- **Thick lines** = Strong weights (important connections)\n",
    "- **Bright neurons** = High activation (neuron is \"firing\")\n",
    "- **Dark neurons** = Low activation (neuron is quiet)\n",
    "\n",
    "---\n",
    "\n",
    "Try experimenting with different:\n",
    "- Network architectures (number of layers, neurons per layer)\n",
    "- Learning rates (faster vs. slower learning)\n",
    "- Activation functions (sigmoid, relu, tanh)\n",
    "- Custom inputs to see how predictions change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  INTERACTIVE NEURAL NETWORK VISUALIZATION\")\n",
    "print(\"  Inspired by 3Blue1Brown\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nFeatures:\")\n",
    "print(\"  ‚úì Real-time network visualization\")\n",
    "print(\"  ‚úì Interactive training controls\")\n",
    "print(\"  ‚úì Step-by-step forward propagation\")\n",
    "print(\"  ‚úì Weight evolution visualization\")\n",
    "print(\"  ‚úì Custom input testing\")\n",
    "print(\"  ‚úì Flexible network architecture\")\n",
    "print(\"\\nTry modifying the code to:\")\n",
    "print(\"  - Add more hidden layers\")\n",
    "print(\"  - Change activation functions\")\n",
    "print(\"  - Test with different datasets\")\n",
    "print(\"  - Create your own visualizations!\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
